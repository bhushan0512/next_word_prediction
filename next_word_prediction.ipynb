{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhushan0512/next_word_prediction/blob/main/next_word_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DEE5Q54FIgmz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Read the text file\n",
        "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2rAqwGjALTDb"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dQxLuWghLVrG"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DWim9ngfLYkb"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M7-0_1bZLb1-"
      },
      "outputs": [],
      "source": [
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BWhQ7Ps8Lhca"
      },
      "outputs": [],
      "source": [
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcjpegjBLh2L",
        "outputId": "a32ad2cc-cfae-478b-fff4-caa7866bb46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 17, 100)           820000    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 150)               150600    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8200)              1238200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,208,800\n",
            "Trainable params: 2,208,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FZXEPcLiEP",
        "outputId": "d8b6887e-57ef-4d5f-e086-89490551a4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3010/3010 [==============================] - 214s 70ms/step - loss: 6.2462 - accuracy: 0.0758\n",
            "Epoch 2/100\n",
            "3010/3010 [==============================] - 209s 70ms/step - loss: 5.5328 - accuracy: 0.1226\n",
            "Epoch 3/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 5.1472 - accuracy: 0.1458\n",
            "Epoch 4/100\n",
            "3010/3010 [==============================] - 206s 68ms/step - loss: 4.8173 - accuracy: 0.1634\n",
            "Epoch 5/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 4.5098 - accuracy: 0.1798\n",
            "Epoch 6/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 4.2208 - accuracy: 0.1998\n",
            "Epoch 7/100\n",
            "3010/3010 [==============================] - 204s 68ms/step - loss: 3.9470 - accuracy: 0.2264\n",
            "Epoch 8/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 3.6889 - accuracy: 0.2554\n",
            "Epoch 9/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 3.4463 - accuracy: 0.2892\n",
            "Epoch 10/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 3.2225 - accuracy: 0.3209\n",
            "Epoch 11/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 3.0127 - accuracy: 0.3566\n",
            "Epoch 12/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 2.8227 - accuracy: 0.3900\n",
            "Epoch 13/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 2.6465 - accuracy: 0.4224\n",
            "Epoch 14/100\n",
            "3010/3010 [==============================] - 203s 68ms/step - loss: 2.4823 - accuracy: 0.4540\n",
            "Epoch 15/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 2.3341 - accuracy: 0.4831\n",
            "Epoch 16/100\n",
            "3010/3010 [==============================] - 210s 70ms/step - loss: 2.1941 - accuracy: 0.5134\n",
            "Epoch 17/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 2.0696 - accuracy: 0.5384\n",
            "Epoch 18/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 1.9512 - accuracy: 0.5616\n",
            "Epoch 19/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 1.8441 - accuracy: 0.5854\n",
            "Epoch 20/100\n",
            "3010/3010 [==============================] - 210s 70ms/step - loss: 1.7442 - accuracy: 0.6062\n",
            "Epoch 21/100\n",
            "3010/3010 [==============================] - 206s 68ms/step - loss: 1.6527 - accuracy: 0.6268\n",
            "Epoch 22/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 1.5703 - accuracy: 0.6449\n",
            "Epoch 23/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 1.4941 - accuracy: 0.6618\n",
            "Epoch 24/100\n",
            "3010/3010 [==============================] - 210s 70ms/step - loss: 1.4220 - accuracy: 0.6786\n",
            "Epoch 25/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 1.3575 - accuracy: 0.6917\n",
            "Epoch 26/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 1.2974 - accuracy: 0.7027\n",
            "Epoch 27/100\n",
            "3010/3010 [==============================] - 209s 69ms/step - loss: 1.2425 - accuracy: 0.7154\n",
            "Epoch 28/100\n",
            "3010/3010 [==============================] - 215s 72ms/step - loss: 1.1912 - accuracy: 0.7276\n",
            "Epoch 29/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 1.1462 - accuracy: 0.7370\n",
            "Epoch 30/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 1.1013 - accuracy: 0.7469\n",
            "Epoch 31/100\n",
            "3010/3010 [==============================] - 193s 64ms/step - loss: 1.0638 - accuracy: 0.7542\n",
            "Epoch 32/100\n",
            "3010/3010 [==============================] - 198s 66ms/step - loss: 1.0262 - accuracy: 0.7633\n",
            "Epoch 33/100\n",
            "3010/3010 [==============================] - 194s 65ms/step - loss: 0.9903 - accuracy: 0.7716\n",
            "Epoch 34/100\n",
            "3010/3010 [==============================] - 196s 65ms/step - loss: 0.9609 - accuracy: 0.7777\n",
            "Epoch 35/100\n",
            "3010/3010 [==============================] - 200s 66ms/step - loss: 0.9318 - accuracy: 0.7844\n",
            "Epoch 36/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 0.9040 - accuracy: 0.7896\n",
            "Epoch 37/100\n",
            "3010/3010 [==============================] - 197s 66ms/step - loss: 0.8751 - accuracy: 0.7971\n",
            "Epoch 38/100\n",
            "3010/3010 [==============================] - 198s 66ms/step - loss: 0.8559 - accuracy: 0.8006\n",
            "Epoch 39/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 0.8321 - accuracy: 0.8060\n",
            "Epoch 40/100\n",
            "3010/3010 [==============================] - 206s 68ms/step - loss: 0.8129 - accuracy: 0.8110\n",
            "Epoch 41/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 0.7935 - accuracy: 0.8143\n",
            "Epoch 42/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.7744 - accuracy: 0.8172\n",
            "Epoch 43/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.7643 - accuracy: 0.8186\n",
            "Epoch 44/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.7458 - accuracy: 0.8238\n",
            "Epoch 45/100\n",
            "3010/3010 [==============================] - 203s 67ms/step - loss: 0.7320 - accuracy: 0.8256\n",
            "Epoch 46/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 0.7202 - accuracy: 0.8286\n",
            "Epoch 47/100\n",
            "3010/3010 [==============================] - 211s 70ms/step - loss: 0.7064 - accuracy: 0.8321\n",
            "Epoch 48/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 0.6944 - accuracy: 0.8334\n",
            "Epoch 49/100\n",
            "3010/3010 [==============================] - 209s 69ms/step - loss: 0.6840 - accuracy: 0.8366\n",
            "Epoch 50/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 0.6742 - accuracy: 0.8382\n",
            "Epoch 51/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 0.6620 - accuracy: 0.8417\n",
            "Epoch 52/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 0.6577 - accuracy: 0.8416\n",
            "Epoch 53/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.6502 - accuracy: 0.8426\n",
            "Epoch 54/100\n",
            "3010/3010 [==============================] - 206s 68ms/step - loss: 0.6443 - accuracy: 0.8432\n",
            "Epoch 55/100\n",
            "3010/3010 [==============================] - 206s 68ms/step - loss: 0.6323 - accuracy: 0.8465\n",
            "Epoch 56/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 0.6285 - accuracy: 0.8470\n",
            "Epoch 57/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.6216 - accuracy: 0.8481\n",
            "Epoch 58/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.6135 - accuracy: 0.8500\n",
            "Epoch 59/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.6097 - accuracy: 0.8499\n",
            "Epoch 60/100\n",
            "3010/3010 [==============================] - 206s 68ms/step - loss: 0.6060 - accuracy: 0.8500\n",
            "Epoch 61/100\n",
            "3010/3010 [==============================] - 209s 69ms/step - loss: 0.5966 - accuracy: 0.8531\n",
            "Epoch 62/100\n",
            "3010/3010 [==============================] - 204s 68ms/step - loss: 0.5927 - accuracy: 0.8538\n",
            "Epoch 63/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 0.5906 - accuracy: 0.8540\n",
            "Epoch 64/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 0.5897 - accuracy: 0.8531\n",
            "Epoch 65/100\n",
            "3010/3010 [==============================] - 203s 67ms/step - loss: 0.5865 - accuracy: 0.8536\n",
            "Epoch 66/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.5770 - accuracy: 0.8561\n",
            "Epoch 67/100\n",
            "3010/3010 [==============================] - 210s 70ms/step - loss: 0.5781 - accuracy: 0.8554\n",
            "Epoch 68/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 0.5775 - accuracy: 0.8543\n",
            "Epoch 69/100\n",
            "3010/3010 [==============================] - 210s 70ms/step - loss: 0.5689 - accuracy: 0.8582\n",
            "Epoch 70/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.5715 - accuracy: 0.8559\n",
            "Epoch 71/100\n",
            "3010/3010 [==============================] - 204s 68ms/step - loss: 0.5653 - accuracy: 0.8573\n",
            "Epoch 72/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 0.5627 - accuracy: 0.8575\n",
            "Epoch 73/100\n",
            "3010/3010 [==============================] - 200s 66ms/step - loss: 0.5625 - accuracy: 0.8574\n",
            "Epoch 74/100\n",
            "3010/3010 [==============================] - 200s 67ms/step - loss: 0.5584 - accuracy: 0.8584\n",
            "Epoch 75/100\n",
            "3010/3010 [==============================] - 203s 68ms/step - loss: 0.5533 - accuracy: 0.8606\n",
            "Epoch 76/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.5543 - accuracy: 0.8591\n",
            "Epoch 77/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.5478 - accuracy: 0.8613\n",
            "Epoch 78/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 0.5538 - accuracy: 0.8580\n",
            "Epoch 79/100\n",
            "3010/3010 [==============================] - 203s 68ms/step - loss: 0.5469 - accuracy: 0.8598\n",
            "Epoch 80/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.5450 - accuracy: 0.8602\n",
            "Epoch 81/100\n",
            "3010/3010 [==============================] - 201s 67ms/step - loss: 0.5447 - accuracy: 0.8604\n",
            "Epoch 82/100\n",
            "3010/3010 [==============================] - 204s 68ms/step - loss: 0.5428 - accuracy: 0.8607\n",
            "Epoch 83/100\n",
            "3010/3010 [==============================] - 204s 68ms/step - loss: 0.5373 - accuracy: 0.8618\n",
            "Epoch 84/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.5482 - accuracy: 0.8595\n",
            "Epoch 85/100\n",
            "3010/3010 [==============================] - 203s 67ms/step - loss: 0.5330 - accuracy: 0.8629\n",
            "Epoch 86/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 0.5398 - accuracy: 0.8615\n",
            "Epoch 87/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.5413 - accuracy: 0.8597\n",
            "Epoch 88/100\n",
            "3010/3010 [==============================] - 204s 68ms/step - loss: 0.5353 - accuracy: 0.8608\n",
            "Epoch 89/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 0.5299 - accuracy: 0.8626\n",
            "Epoch 90/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.5316 - accuracy: 0.8617\n",
            "Epoch 91/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.5308 - accuracy: 0.8621\n",
            "Epoch 92/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 0.5314 - accuracy: 0.8609\n",
            "Epoch 93/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 0.5265 - accuracy: 0.8629\n",
            "Epoch 94/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 0.5329 - accuracy: 0.8601\n",
            "Epoch 95/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.5214 - accuracy: 0.8643\n",
            "Epoch 96/100\n",
            "3010/3010 [==============================] - 206s 68ms/step - loss: 0.5304 - accuracy: 0.8610\n",
            "Epoch 97/100\n",
            "3010/3010 [==============================] - 202s 67ms/step - loss: 0.5313 - accuracy: 0.8603\n",
            "Epoch 98/100\n",
            "3010/3010 [==============================] - 210s 70ms/step - loss: 0.5244 - accuracy: 0.8625\n",
            "Epoch 99/100\n",
            "3010/3010 [==============================] - 209s 69ms/step - loss: 0.5234 - accuracy: 0.8634\n",
            "Epoch 100/100\n",
            "3010/3010 [==============================] - 209s 69ms/step - loss: 0.5233 - accuracy: 0.8627\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7d661671b7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DQTrgTIrLicH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7927f4-12f4-4fee-8b41-baa7c52ebfa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "I would like to fess key prisoner replace bisulphate leaking excavating fountain gathering\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"I would like to\"\n",
        "next_words = 9\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "id": "sJoamsAVQuhr",
        "outputId": "b4efc8f3-760f-4b05-83ea-98c0a56dbd66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "ecjEr9--RIuL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://bhushan0512:Bhushan@0123@github.com/bhushan0512/next_word_prediction.git\n",
        "!git remote -v\n",
        "!git branch -M main\n",
        "!git config --global user.email \"bhushanabangera687@gmail.com.com\"\n",
        "!git config --global user.name \"Bhushan\"\n",
        "!git add .\n",
        "!git commit -m \"first commit\"\n",
        "!git push -u origin main"
      ],
      "metadata": {
        "id": "x_Ko4NjPRPQ4",
        "outputId": "bf1e72aa-70b7-4594-d9cf-f1862385c23c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: remote origin already exists.\n",
            "origin\thttps://github.com/bhushan0512/next_word_prediction.git (fetch)\n",
            "origin\thttps://github.com/bhushan0512/next_word_prediction.git (push)\n",
            "On branch main\n",
            "nothing to commit, working tree clean\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCzx3ZILwv0Z83QbB7MbVT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}